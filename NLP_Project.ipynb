{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaelhlew/nlp-project/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle"
      ],
      "metadata": {
        "id": "LTMpy3F0SFWr"
      },
      "id": "LTMpy3F0SFWr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "_rJZMVjCgCJJ"
      },
      "id": "_rJZMVjCgCJJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "JSJ6fRx5gbzF"
      },
      "id": "JSJ6fRx5gbzF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "9CZOD5xzgo6l"
      },
      "id": "9CZOD5xzgo6l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d leadbest/googlenewsvectorsnegative300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJy-QcCFhCPZ",
        "outputId": "5460cb54-eb13-49e9-f4ec-7dd92d96ebb9"
      },
      "id": "nJy-QcCFhCPZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading googlenewsvectorsnegative300.zip to /content\n",
            "100% 3.16G/3.17G [00:33<00:00, 155MB/s]\n",
            "100% 3.17G/3.17G [00:33<00:00, 102MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip googlenewsvectorsnegative300.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXEZ0g3UhZWC",
        "outputId": "70b680df-2882-45d1-dc54-91a371ccdacf"
      },
      "id": "DXEZ0g3UhZWC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  googlenewsvectorsnegative300.zip\n",
            "  inflating: GoogleNews-vectors-negative300.bin  \n",
            "  inflating: GoogleNews-vectors-negative300.bin.gz  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c42d1503",
      "metadata": {
        "id": "c42d1503",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af5c243-2aff-4ab1-cab2-b5b78fdb0677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import NMF, PCA, TruncatedSVD, FastICA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_validate, KFold, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from nltk.util import ngrams\n",
        "import cgi\n",
        "import nltk\n",
        "import html\n",
        "#import readability \n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxZ70qbF3ayD",
        "outputId": "16327ffc-eaaa-4771-c22d-79c0b227a856"
      },
      "id": "yxZ70qbF3ayD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "954ac9fb",
      "metadata": {
        "id": "954ac9fb"
      },
      "source": [
        "**Data Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c9351a7",
      "metadata": {
        "id": "1c9351a7"
      },
      "outputs": [],
      "source": [
        "biden = pd.read_csv(\"biden_filtered.csv\")\n",
        "trump = pd.read_csv(\"trump_filtered.csv\")\n",
        "biden['author'] = \"Biden\"\n",
        "trump['author'] = \"Trump\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "919524f9",
      "metadata": {
        "id": "919524f9"
      },
      "outputs": [],
      "source": [
        "trump = trump[trump.columns[1:]]\n",
        "biden = biden[biden.columns[1:]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4f101a16",
      "metadata": {
        "id": "4f101a16"
      },
      "outputs": [],
      "source": [
        "tweets = pd.concat([biden, trump])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "87b04ec2",
      "metadata": {
        "id": "87b04ec2"
      },
      "outputs": [],
      "source": [
        "class TwitterCleaner:\n",
        "    def __init__(self, df, column):\n",
        "        self.df = df\n",
        "        self.column = column\n",
        "\n",
        "    def decode_xml_entities(self, text):\n",
        "        text = html.unescape(text)\n",
        "        text = str(text)\n",
        "        return text\n",
        "\n",
        "    def clean_text(self, lemmatize=False, stem=False, remove_stopwords=True):\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        self.df[self.column] = self.df[self.column].apply(lambda x: str(x))\n",
        "        clean_list = []\n",
        "        for text in self.df[self.column]:\n",
        "            # remove URLs\n",
        "            text = re.sub(r'http\\S+','', text)  \n",
        "\n",
        "            # remove '@' twitter mentions\n",
        "            text = re.sub(r'@[A-Za-z0-9_]+','', text) \n",
        "\n",
        "            # XML to characters\n",
        "            text = self.decode_xml_entities(text)\n",
        "\n",
        "            # lowercase the text\n",
        "            text = text.lower() \n",
        "            words = word_tokenize(text)\n",
        "\n",
        "            clean_words = []\n",
        "\n",
        "            # stemming / lemmatization\n",
        "\n",
        "            for word in words:\n",
        "                if remove_stopwords == True and word in stop_words:\n",
        "                    continue\n",
        "                if lemmatize == True: \n",
        "                    lemmatizer = WordNetLemmatizer()\n",
        "                    word = lemmatizer.lemmatize(word)\n",
        "                if stem == True: \n",
        "                    stemmer = PorterStemmer()\n",
        "                    word = stemmer.stem(word)\n",
        "                clean_words.append(word)\n",
        "\n",
        "            clean_text = ' '.join(clean_words)\n",
        "            clean_list.append(clean_text)\n",
        "\n",
        "        if lemmatize:\n",
        "            if remove_stopwords:\n",
        "                self.df[f\"{self.column}_clean_lemmatized_stopwords\"] = clean_list\n",
        "            else:\n",
        "                self.df[f\"{self.column}_clean_lemmatized\"] = clean_list\n",
        "        elif stem:\n",
        "            if remove_stopwords:\n",
        "                self.df[f\"{self.column}_clean_stemmed_stopwords\"] = clean_list\n",
        "            else:\n",
        "                self.df[f\"{self.column}_clean_stemmed\"] = clean_list\n",
        "        else:\n",
        "            if remove_stopwords:\n",
        "                self.df[f\"{self.column}_clean_stopwords\"] = clean_list\n",
        "            else:\n",
        "                self.df[f\"{self.column}_clean\"] = clean_list\n",
        "\n",
        "        return self.df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "13c6f6a9",
      "metadata": {
        "id": "13c6f6a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "740cb370-84b4-4384-9bea-a7470177717e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text   likes  retweets  \\\n",
              "0     Every single human being deserves to be treate...   11574      2423   \n",
              "1     With just over one month until the Iowa Caucus...    1457       368   \n",
              "2     This election is about the soul of our nation ...   44886     10192   \n",
              "3     Every day that Donald Trump remains in the Whi...    9581      2005   \n",
              "4     It was a privilege to work with @JulianCastro ...   17156      2284   \n",
              "...                                                 ...     ...       ...   \n",
              "6403  Iran never won a war, but never lost a negotia...  303007     57253   \n",
              "6404  Thank you to the @dcexaminer Washington Examin...   35044      9213   \n",
              "6405  One of my greatest honors was to have gotten C...   56731     12761   \n",
              "6406  Just signed an order to support the workers of...  176289     36001   \n",
              "6407  Suburban women want Safety & Security. Joe Bid...   95169     19545   \n",
              "\n",
              "                timestamp            id author  \\\n",
              "0     2020-01-01 18:35:00  1.212442e+18  Biden   \n",
              "1     2020-01-02 00:01:00  1.212524e+18  Biden   \n",
              "2     2020-01-02 01:05:00  1.212540e+18  Biden   \n",
              "3     2020-01-02 02:07:00  1.212556e+18  Biden   \n",
              "4     2020-01-02 16:10:00  1.212768e+18  Biden   \n",
              "...                   ...           ...    ...   \n",
              "6403  2020-01-03 12:44:30  1.213079e+18  Trump   \n",
              "6404  2020-01-01 01:03:15  1.212177e+18  Trump   \n",
              "6405  2020-01-01 00:55:01  1.212175e+18  Trump   \n",
              "6406  2020-10-22 21:04:21  1.319384e+18  Trump   \n",
              "6407  2020-10-22 18:31:46  1.319346e+18  Trump   \n",
              "\n",
              "                        text_clean_lemmatized_stopwords  \\\n",
              "0     every single human deserves treated dignity . ...   \n",
              "1     one month iowa caucus , need hand deck talk fo...   \n",
              "2     election soul nation — donald trump poison soul .   \n",
              "3     every day donald trump remains white house put...   \n",
              "4     privilege work obama administration , true hon...   \n",
              "...                                                 ...   \n",
              "6403          iran never war , never lost negotiation !   \n",
              "6404  thank washington examiner . list growing every...   \n",
              "6405  one greatest honor gotten choice approved grea...   \n",
              "6406  signed order support worker delphi corporation...   \n",
              "6407  suburban woman want safety & security . joe bi...   \n",
              "\n",
              "                           text_clean_stemmed_stopwords  \n",
              "0     everi singl human deserv treat digniti . every...  \n",
              "1     one month iowa caucu , need hand deck talk fol...  \n",
              "2        elect soul nation — donald trump poison soul .  \n",
              "3     everi day donald trump remain white hous put f...  \n",
              "4     privileg work obama administr , true honor tal...  \n",
              "...                                                 ...  \n",
              "6403               iran never war , never lost negoti !  \n",
              "6404    thank washington examin . list grow everi day !  \n",
              "6405  one greatest honor gotten choic approv great v...  \n",
              "6406  sign order support worker delphi corpor make s...  \n",
              "6407  suburban women want safeti & secur . joe biden...  \n",
              "\n",
              "[9335 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b716d75f-bb59-4a45-af03-a53038c44733\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>likes</th>\n",
              "      <th>retweets</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>text_clean_lemmatized_stopwords</th>\n",
              "      <th>text_clean_stemmed_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Every single human being deserves to be treate...</td>\n",
              "      <td>11574</td>\n",
              "      <td>2423</td>\n",
              "      <td>2020-01-01 18:35:00</td>\n",
              "      <td>1.212442e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>every single human deserves treated dignity . ...</td>\n",
              "      <td>everi singl human deserv treat digniti . every...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>With just over one month until the Iowa Caucus...</td>\n",
              "      <td>1457</td>\n",
              "      <td>368</td>\n",
              "      <td>2020-01-02 00:01:00</td>\n",
              "      <td>1.212524e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>one month iowa caucus , need hand deck talk fo...</td>\n",
              "      <td>one month iowa caucu , need hand deck talk fol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This election is about the soul of our nation ...</td>\n",
              "      <td>44886</td>\n",
              "      <td>10192</td>\n",
              "      <td>2020-01-02 01:05:00</td>\n",
              "      <td>1.212540e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>election soul nation — donald trump poison soul .</td>\n",
              "      <td>elect soul nation — donald trump poison soul .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Every day that Donald Trump remains in the Whi...</td>\n",
              "      <td>9581</td>\n",
              "      <td>2005</td>\n",
              "      <td>2020-01-02 02:07:00</td>\n",
              "      <td>1.212556e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>every day donald trump remains white house put...</td>\n",
              "      <td>everi day donald trump remain white hous put f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It was a privilege to work with @JulianCastro ...</td>\n",
              "      <td>17156</td>\n",
              "      <td>2284</td>\n",
              "      <td>2020-01-02 16:10:00</td>\n",
              "      <td>1.212768e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>privilege work obama administration , true hon...</td>\n",
              "      <td>privileg work obama administr , true honor tal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6403</th>\n",
              "      <td>Iran never won a war, but never lost a negotia...</td>\n",
              "      <td>303007</td>\n",
              "      <td>57253</td>\n",
              "      <td>2020-01-03 12:44:30</td>\n",
              "      <td>1.213079e+18</td>\n",
              "      <td>Trump</td>\n",
              "      <td>iran never war , never lost negotiation !</td>\n",
              "      <td>iran never war , never lost negoti !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6404</th>\n",
              "      <td>Thank you to the @dcexaminer Washington Examin...</td>\n",
              "      <td>35044</td>\n",
              "      <td>9213</td>\n",
              "      <td>2020-01-01 01:03:15</td>\n",
              "      <td>1.212177e+18</td>\n",
              "      <td>Trump</td>\n",
              "      <td>thank washington examiner . list growing every...</td>\n",
              "      <td>thank washington examin . list grow everi day !</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6405</th>\n",
              "      <td>One of my greatest honors was to have gotten C...</td>\n",
              "      <td>56731</td>\n",
              "      <td>12761</td>\n",
              "      <td>2020-01-01 00:55:01</td>\n",
              "      <td>1.212175e+18</td>\n",
              "      <td>Trump</td>\n",
              "      <td>one greatest honor gotten choice approved grea...</td>\n",
              "      <td>one greatest honor gotten choic approv great v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6406</th>\n",
              "      <td>Just signed an order to support the workers of...</td>\n",
              "      <td>176289</td>\n",
              "      <td>36001</td>\n",
              "      <td>2020-10-22 21:04:21</td>\n",
              "      <td>1.319384e+18</td>\n",
              "      <td>Trump</td>\n",
              "      <td>signed order support worker delphi corporation...</td>\n",
              "      <td>sign order support worker delphi corpor make s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6407</th>\n",
              "      <td>Suburban women want Safety &amp; Security. Joe Bid...</td>\n",
              "      <td>95169</td>\n",
              "      <td>19545</td>\n",
              "      <td>2020-10-22 18:31:46</td>\n",
              "      <td>1.319346e+18</td>\n",
              "      <td>Trump</td>\n",
              "      <td>suburban woman want safety &amp; security . joe bi...</td>\n",
              "      <td>suburban women want safeti &amp; secur . joe biden...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9335 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b716d75f-bb59-4a45-af03-a53038c44733')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b716d75f-bb59-4a45-af03-a53038c44733 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b716d75f-bb59-4a45-af03-a53038c44733');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tc = TwitterCleaner(tweets, 'text')\n",
        "tweets = tc.clean_text(lemmatize = True, stem = False, remove_stopwords = True)\n",
        "tweets = tc.clean_text(lemmatize = False, stem = True, remove_stopwords = True)\n",
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.groupby(['author'])['likes'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPP9bJbxdLjQ",
        "outputId": "6ddd2544-698c-4595-ab57-9f95c74b6351"
      },
      "id": "NPP9bJbxdLjQ",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author\n",
              "Biden     63039.107277\n",
              "Trump    117850.719101\n",
              "Name: likes, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.groupby(['author'])['retweets'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBU79C-neVXF",
        "outputId": "d5055b39-cf54-43e7-9395-2efdb0b9b2bc"
      },
      "id": "aBU79C-neVXF",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author\n",
              "Biden    11243.899898\n",
              "Trump    25571.497035\n",
              "Name: retweets, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52442f1b",
      "metadata": {
        "id": "52442f1b"
      },
      "source": [
        "**Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e13879",
      "metadata": {
        "id": "b2e13879"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sentiment_score(sentence):\n",
        "    score = analyzer.polarity_scores(sentence)\n",
        "    return score['compound']\n",
        "\n",
        "tweets['sentiment_scores'] = [sentiment_score(sentence) for sentence in tweets['text_clean_lemmatized_stopwords']]\n",
        "tweets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "# function to calculate subjectivity\n",
        "def getSubjectivity(review):\n",
        "    return TextBlob(review).sentiment.subjectivity\n",
        "    # function to calculate polarity\n",
        "def getPolarity(review):\n",
        "    return TextBlob(review).sentiment.polarity\n",
        "# function to analyze the reviews\n",
        "def analysis(score):\n",
        "    if score < 0:\n",
        "        return '0' #Negative \n",
        "    else:\n",
        "        return '1' #Positive"
      ],
      "metadata": {
        "id": "ANY_6EFfelWd"
      },
      "id": "ANY_6EFfelWd",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['Subjectivity'] = tweets['text_clean_lemmatized_stopwords'].apply(getSubjectivity) \n",
        "tweets['Polarity'] = tweets['text_clean_lemmatized_stopwords'].apply(getPolarity) \n",
        "tweets['Analysis'] = tweets['Polarity'].apply(analysis)\n",
        "tweets.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "i30LWIcOe_kF",
        "outputId": "b6091073-7e1c-4bb8-bb28-b658de0b86c5"
      },
      "id": "i30LWIcOe_kF",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  likes  retweets  \\\n",
              "0  Every single human being deserves to be treate...  11574      2423   \n",
              "1  With just over one month until the Iowa Caucus...   1457       368   \n",
              "2  This election is about the soul of our nation ...  44886     10192   \n",
              "3  Every day that Donald Trump remains in the Whi...   9581      2005   \n",
              "4  It was a privilege to work with @JulianCastro ...  17156      2284   \n",
              "\n",
              "             timestamp            id author  \\\n",
              "0  2020-01-01 18:35:00  1.212442e+18  Biden   \n",
              "1  2020-01-02 00:01:00  1.212524e+18  Biden   \n",
              "2  2020-01-02 01:05:00  1.212540e+18  Biden   \n",
              "3  2020-01-02 02:07:00  1.212556e+18  Biden   \n",
              "4  2020-01-02 16:10:00  1.212768e+18  Biden   \n",
              "\n",
              "                     text_clean_lemmatized_stopwords  \\\n",
              "0  every single human deserves treated dignity . ...   \n",
              "1  one month iowa caucus , need hand deck talk fo...   \n",
              "2  election soul nation — donald trump poison soul .   \n",
              "3  every day donald trump remains white house put...   \n",
              "4  privilege work obama administration , true hon...   \n",
              "\n",
              "                        text_clean_stemmed_stopwords  Subjectivity  Polarity  \\\n",
              "0  everi singl human deserv treat digniti . every...      0.452381 -0.295238   \n",
              "1  one month iowa caucu , need hand deck talk fol...      0.000000  0.000000   \n",
              "2     elect soul nation — donald trump poison soul .      0.000000  0.000000   \n",
              "3  everi day donald trump remain white hous put f...      0.075000  0.033333   \n",
              "4  privileg work obama administr , true honor tal...      0.516667  0.350000   \n",
              "\n",
              "  Analysis  \n",
              "0        0  \n",
              "1        1  \n",
              "2        1  \n",
              "3        1  \n",
              "4        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c335405-b12c-4c70-adae-7ba4c3a7f090\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>likes</th>\n",
              "      <th>retweets</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>id</th>\n",
              "      <th>author</th>\n",
              "      <th>text_clean_lemmatized_stopwords</th>\n",
              "      <th>text_clean_stemmed_stopwords</th>\n",
              "      <th>Subjectivity</th>\n",
              "      <th>Polarity</th>\n",
              "      <th>Analysis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Every single human being deserves to be treate...</td>\n",
              "      <td>11574</td>\n",
              "      <td>2423</td>\n",
              "      <td>2020-01-01 18:35:00</td>\n",
              "      <td>1.212442e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>every single human deserves treated dignity . ...</td>\n",
              "      <td>everi singl human deserv treat digniti . every...</td>\n",
              "      <td>0.452381</td>\n",
              "      <td>-0.295238</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>With just over one month until the Iowa Caucus...</td>\n",
              "      <td>1457</td>\n",
              "      <td>368</td>\n",
              "      <td>2020-01-02 00:01:00</td>\n",
              "      <td>1.212524e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>one month iowa caucus , need hand deck talk fo...</td>\n",
              "      <td>one month iowa caucu , need hand deck talk fol...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This election is about the soul of our nation ...</td>\n",
              "      <td>44886</td>\n",
              "      <td>10192</td>\n",
              "      <td>2020-01-02 01:05:00</td>\n",
              "      <td>1.212540e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>election soul nation — donald trump poison soul .</td>\n",
              "      <td>elect soul nation — donald trump poison soul .</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Every day that Donald Trump remains in the Whi...</td>\n",
              "      <td>9581</td>\n",
              "      <td>2005</td>\n",
              "      <td>2020-01-02 02:07:00</td>\n",
              "      <td>1.212556e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>every day donald trump remains white house put...</td>\n",
              "      <td>everi day donald trump remain white hous put f...</td>\n",
              "      <td>0.075000</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It was a privilege to work with @JulianCastro ...</td>\n",
              "      <td>17156</td>\n",
              "      <td>2284</td>\n",
              "      <td>2020-01-02 16:10:00</td>\n",
              "      <td>1.212768e+18</td>\n",
              "      <td>Biden</td>\n",
              "      <td>privilege work obama administration , true hon...</td>\n",
              "      <td>privileg work obama administr , true honor tal...</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.350000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c335405-b12c-4c70-adae-7ba4c3a7f090')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c335405-b12c-4c70-adae-7ba4c3a7f090 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c335405-b12c-4c70-adae-7ba4c3a7f090');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c10d7e",
      "metadata": {
        "id": "e5c10d7e",
        "outputId": "54342906-f798-40b0-fc8a-f3372ea35fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average sentiment score for Biden: 0.17794974376494702\n"
          ]
        }
      ],
      "source": [
        "print(\"Average sentiment score for Biden: \" + str(tweets[tweets['author'] == \"Biden\"]['sentiment_scores'].mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798fa147",
      "metadata": {
        "id": "798fa147",
        "outputId": "e8854790-99e0-4ceb-806c-e624420689f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average sentiment score for Trump: 0.1705028870162297\n"
          ]
        }
      ],
      "source": [
        "print(\"Average sentiment score for Trump: \" + str(tweets[tweets['author'] == \"Trump\"]['sentiment_scores'].mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8ac51c",
      "metadata": {
        "id": "3f8ac51c"
      },
      "outputs": [],
      "source": [
        "# Basically the same sentiment score "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN\n",
        "import random\n",
        "random.seed(123)\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "#NN imports\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "Z1WPnjFZ0y1D"
      },
      "id": "Z1WPnjFZ0y1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Recoding author data into binary\n",
        "tweets['author_dummy'] = np.where(tweets['author'] == \"Biden\", 1, 0)\n",
        "tweets.head()"
      ],
      "metadata": {
        "id": "ZQe83226Slhc"
      },
      "id": "ZQe83226Slhc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
        "print('done loading Word2Vec')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdt-bcwofuHH",
        "outputId": "76a30e30-24e6-48ca-bb86-3f10cdc01e93"
      },
      "id": "pdt-bcwofuHH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done loading Word2Vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_vectorize(tweets):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    vectorized_data = []\n",
        "    expected = []\n",
        "    for sample in tweets:\n",
        "        tokens = tokenizer.tokenize(sample[1])\n",
        "        sample_vecs = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                sample_vecs.append(word_vectors[token])\n",
        "\n",
        "            except KeyError:\n",
        "                pass  # No matching token in the Google w2v vocab\n",
        "            \n",
        "        vectorized_data.append(sample_vecs)\n",
        "\n",
        "    return vectorized_data"
      ],
      "metadata": {
        "id": "DXSi2MOJi1ra"
      },
      "id": "DXSi2MOJi1ra",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_expected(tweets):\n",
        "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
        "    expected = ['author_dummy']\n",
        "    for sample in tweets:\n",
        "        expected.append(sample[0])\n",
        "    return expected"
      ],
      "metadata": {
        "id": "C3-laSM2kRTT"
      },
      "id": "C3-laSM2kRTT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_data = tokenize_and_vectorize(tweets)\n",
        "expected = collect_expected(tweets)"
      ],
      "metadata": {
        "id": "A_PYDeYSjVgb"
      },
      "id": "A_PYDeYSjVgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "split_point = int(len(vectorized_data)*.7)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ],
      "metadata": {
        "id": "Wf4jTZtUEpI1"
      },
      "id": "Wf4jTZtUEpI1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 400\n",
        "embedding_dims = 300  "
      ],
      "metadata": {
        "id": "6q1K9aXDld_V"
      },
      "id": "6q1K9aXDld_V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Must manually pad/truncate\n",
        "\n",
        "def pad_trunc(data, maxlen):\n",
        "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
        "    new_data = []\n",
        "\n",
        "    # Create a vector of 0's the length of our word vectors\n",
        "    zero_vector = []\n",
        "    for _ in range(len(data[0][0])):\n",
        "        zero_vector.append(0.0)\n",
        "\n",
        "    for sample in data:\n",
        " \n",
        "        if len(sample) > maxlen:\n",
        "            temp = sample[:maxlen]\n",
        "        elif len(sample) < maxlen:\n",
        "            temp = sample\n",
        "            additional_elems = maxlen - len(sample)\n",
        "            for _ in range(additional_elems):\n",
        "                temp.append(zero_vector)\n",
        "        else:\n",
        "            temp = sample\n",
        "        new_data.append(temp)\n",
        "    return new_data"
      ],
      "metadata": {
        "id": "BPzmrD6YlhaW"
      },
      "id": "BPzmrD6YlhaW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)\n",
        "\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "1OZ0YnU4mLC0"
      },
      "id": "1OZ0YnU4mLC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
        "filters = 250           # Number of filters we will train\n",
        "kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
        "hidden_dims = 250       # \\\n",
        "epochs = 3  "
      ],
      "metadata": {
        "id": "sTI4CygWmhcx"
      },
      "id": "sTI4CygWmhcx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1,\n",
        "                 input_shape=(maxlen, embedding_dims)))\n",
        "# we use max pooling:\n",
        "model.add(GlobalMaxPooling1D())\n",
        "# We add a vanilla hidden layer:\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSo0_RYumtz5",
        "outputId": "b4948dd8-f9d2-4b7b-d37b-b382ded2de45"
      },
      "id": "gSo0_RYumtz5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DOX5ZakTm0XS"
      },
      "id": "DOX5ZakTm0XS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1L_DRuvEm2ji",
        "outputId": "be89c50b-e942-46b9-9967-67013c65175c"
      },
      "id": "1L_DRuvEm2ji",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-ce1528563816>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(x_train, y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           validation_data=(x_test, y_test))\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 687, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/ioloop.py\", line 740, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 821, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 782, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.9/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-114-ce1528563816>\", line 1, in <cell line: 1>\n      model.fit(x_train, y_train,\n    File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.9/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.9/dist-packages/keras/losses.py\", line 2145, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_3664]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}